%\subsubsection{Budget}
%Machine learning models require fine tuning of different parameters, from the selection and testing of multiple methods, to the parameterization of the final architecture. \citep{[REF??]} suggests that every publication in machine learning should include a section on the budget used for the development and training of the method. The budget is the amount of resources used in the data processing, the selection of the model hyper-parameters (HP), and its training.
%
%The most time-consuming task in the present work has been the data preparation, the model setup and debugging and the writing of the SOM visualization routines. All the techniques described in the Methods section \ref{sec:methods} have been coded in python and are freely accessible in the repositories listed in section \ref{sec:repos}. We estimate the effort to bring this work from scratch to a total of 2 persons month. Of these, one person week was dedicated to the testing an selection of different model HPs (autoencoder architecture, feature selection, learning rates, initialization methods, number of epochs for training, selection of data compression method, size of the time windows, etc.).
%
%All classic clustering techniques presented in section \ref{sec:clustering} require only a few lines of code and can be trained in less than a minute on a current generation laptop (e.g. Dell XPS 13 2016 with an Intel Core i7-7500U CPU). The most time consuming task is the training of the autoencoder which can last for up to 3 minutes. The training of the SOM is performed in less than a minute.
%
%\subsubsection{Hyper-Parameter Optimization}
%We are focusing in this work on the use of the SOMs. These require the selection of four main HPs: the size of the lattice, $(m\times n)$, the initial learning rate, $\epsilon_0$HPs are replaced by the constant learning rate, $\epsilon$, and the elasticity, $\eta$. The automatic selection of the best HP for machine learning model is called Hyper-Parameter Optimization (HPO).
%
%We use the library `optuna' \citep{[REF??]} to perform an automatic optimization of the four HPs. The optimization is based on a technique called Tree-structured Parzen Estimator (TPE) \citep{[REF 3 IN OPTUNA PAPER??]}. The objective function of the optimizer was set to minimize the quantization error $Q_E$, the mean inter nodal distance, and the elasticity (or the neighborhood width) of the SOM. After a total of 100 trials (runs of the model using different HPs), the optimizer selected the parameters presented in table \ref{tab:hpo}. The total run-time for a single HPO is in the order of 30 minutes on a home computer. HPO is, understandably, the most expensive procedure.