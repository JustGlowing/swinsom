\paragraph{Budget}
Machine learning models require fine tuning of different parameters, from the selection and testing of multiple methods, to the parameterization of the final architecture. \citep{[REF??]} suggests that every publication in machine learning should include a section on the budget used for the development and training of the method. The budget is the amount of resources used in the data processing, the selection of the model hyper-parameters, and its training.

The most time-consuming task in the present work has been the data preparation, the model setup and debugging and the writing of the SOM visualization routines. All the techniques described in the Methods section \ref{sec:methods} have been coded in python and are freely accessible in the repositories listed in section \ref{sec:repos}. We estimate the effort to bring this work from scratch to a total of 2 persons month. Of these, one person week was dedicated to the testing an selection of different model hyper-parameters (autoencoder architecture, feature selection, learning rates, initialization methods, number of epochs for training, selection of data compression method, size of the time windows, etc.).

All classic clustering techniques presented in section \ref{sec:clustering} require only a few lines of code and can be trained in less than a minute on a current generation laptop (e.g. Dell XPS 13 2016 with an Intel Core i7-7500U CPU). The most time consuming task is the training of the autoencoder which can last for up to 3 minutes. The training of the SOM is performed in less than a minute.

\paragraph{Automatic Optimization}
We are focusing in this work on the use of the SOM/DSOM methods. Four hyper-parameters are extremely important 