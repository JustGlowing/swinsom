\subsubsection{Dimension Reduction using PCA}
Principal Component Analysis (PCA) is a mathematical tool used in data analysis to simplify and extract the most relevant features in the data set. This technique is used to create entries composed of linearly uncorrelated `principal components'. These are the eigenvectors of the covariance matrix $\Sigma$ applied to the centered data, eq.\eqref{eq:covariance}, ordered from the largest to the smallest eigenvalue, $\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_n$, where $\overline{\boldsymbol{X}}$ is the mean value of each original feature, eq.\eqref{eq:xmean}. The projection of the data onto the principal component space ensures a maximal variance on the direction of the first component. Each subsequent principal component is orthogonal to the previous ones and points in the direction of maximal variance in the residual sub-space \citep{Shlens2014}.

\begin{align}
\overline{\boldsymbol{X}} & = \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{X}_i \label{eq:xmean} \\
\Sigma & = \frac{1}{m} \sum_{i=1}^{m} \left( \boldsymbol{X}_i - \overline{\boldsymbol{X}} \right)\left( \boldsymbol{X}_i - \overline{\boldsymbol{X}} \right) \label{eq:covariance}
\end{align}

The PCA transformation creates as many components as features in the original data. However, components with small eigenvalues belong to a dimension where the variance is so small that it is impossible to separate points in the data. It is a general practice in data reduction to keep only the first $k$ components that explain at least a significant portion of the total variance of the data, $\lambda_{i=1..k}/\text{Tr}(\Sigma) > \epsilon$. This allows for a selection of information that will effectively differentiate data points, and for a reduction of the amount of data to process during analysis.

Fig. \ref{fig:dimreduc}-a,b is a scatter plot of all the data points, colored by the X classification, projected on the first three PCA components. The features used to create this figure are presented in section \ref{sec:[WHICH ONE??]}. Panels c and d contain the same data colored by the Z classification. These projections show that some of the empirical solar wind classes are almost linearly separable. However, class 2, ICMEs/Ejecta, is restricted to a small domain in this coordinate system. Panels e and f are 2D histograms of the point distribution in the three main PCA planes. They show that there are multiple clusters of high density points that can be potentially isolated using unsupervised classification techniques. The point distribution shows similar variance in each one of the three PCA components, and present a sharp drop in density at its borders.

\subsubsection{Dimension Reduction Using Autoencoders}
PCA has a limitation: the principal components are a linear combination of the original features of the data. An alternative to data reduction is the use of autoencoders. These are machine learning techniques that can create non-linear combinations of the original features projected on a latent space with less dimensions \citep{Hinton2006}. This is accomplished by creating a system where an encoding function, $\phi$, maps the original data $\boldsymbol{X}$ to a latent space, $\boldsymbol{\mathcal{F}}$, eq.\eqref{eq:encoder}. A decoder function, $\psi$, then maps the latent space back to the original input space \eqref{eq:decoder}. The objective of the autoencoder is to minimize the error between the original data and the data produced by the compression-decompression procedure as shown in eq.\eqref{eq:aeminimization}.

\begin{align}
\phi: & \boldsymbol{X} \rightarrow \boldsymbol{\mathcal{F}} \label{eq:encoder}\\
\psi: & \boldsymbol{\mathcal{F}} \rightarrow \boldsymbol{X} \label{eq:decoder} \\
\phi,\psi = & \underset{\phi,\psi}{\arg \min} \left\lVert \boldsymbol{X} - (\phi \circ \psi) \boldsymbol{X} \right\rVert^2 \label{eq:aeminimization}
\end{align}

Autoencoders can be represented as feed-forward neural networks, where the size of each consecutive layer decreases down to a bottleneck and then expands reaching again the input layer size. An encoded element, $\boldsymbol{z} \in \boldsymbol{\mathcal{F}}$, can be obtained from a data entry, $\boldsymbol{x} \in \boldsymbol{X}$, following the standard neural network function, eq.\eqref{eq:encodex}, where $\boldsymbol{W}$ is the weights matrix, $b$ is the bias, and $\sigma$ is the non-linear activation function.

\begin{align}
\boldsymbol{z} & = \sigma \left( \boldsymbol{W}\boldsymbol{x} + \boldsymbol{b} \right) \label{eq:encodex} \\
\boldsymbol{\hat{x}} & = \sigma' \left( \boldsymbol{W'}\boldsymbol{z} + \boldsymbol{b'} \right) \label{eq:decodez} \\ 
\mathcal{L}(\boldsymbol{x}, \boldsymbol{\hat{x}}) & =  \left\lVert \boldsymbol{x}- \boldsymbol{\hat{x}} \right\rVert^2 \label{eq:aeloss}
\end{align}

The decoding procedure, shown in eq.\eqref{eq:decodez}, transforms $\boldsymbol{z}\rightarrow\boldsymbol{\hat{x}}$, where the prime quantities are associated with the decoder. The loss function, $\mathcal{L}(\boldsymbol{x}, \boldsymbol{\hat{x}})$, is the objective to be minimized by the training of the neural network using gradient descent. Once trained, the vector $\boldsymbol{z}$ is a projection of the input vector $\boldsymbol{x}$ onto the lower dimesion latent space $\boldsymbol{\mathcal{F}}$.

Additional enhancements and variations of this simple autoencoder setup exist in the literature, including multiple regularization techniques to minimize over-fitting \citep{[REF??]}, Variational Autoencoders (VAE) that produce encoded Gaussian distribution functions \citep{[REF??]}, and Generative Adversarial Networks (GAN) that produce new (unseen) data \citep{[REF??]}. In this work we use the most basic form of autoencoders, presented above.

The second row of panels in Fig.\ref{fig:dimreduc}, from g to l, contains the same information as the first row, but with the data set encoded in a three dimensional latent space. Panels g-j show that all empirical solar wind types are easily differentiated, including ICMEs/Ejecta (class 2, in [COLOR??]). This projection also shows that class 4 from the Z classification (non-coronal hole wind) overlaps with class 3 (sector reversal origin), and partially with class 2 (ejecta) from the X classification. Panels k and l show the density of points observed in the latent space. Here again it is possible to observe multiple zones of high concentration, suggesting that multiple types of solar wind are present in the data and that they can be differentiated using an unsupervised classification technique.

\subsubsection{Clustering Techniques}
\label{sec:clustering}
The goal of unsupervised machine learning is to group data points in a limited number of clusters in the N-dimensional space $\Omega\in\Bbb R^N$, where N is the number of features (components) in the data set. Multiple techniques can be used to perform multi-dimensional clustering. We present in Fig. \ref{fig:clustering} three clustering techniques used to classify our 3D reduced data. The panels in the first column shows the data projected in the first and second dimensions of the autoencoded latent space, while the second column shows the data in the first and third dimensions. Panels in columns 3 and 4 contain the same type of projection in the PCA reduced space. Each row corresponds to a different clustering method where the number of classes has been imposed to four. The top four panels (a, b, g, h) were obtained using the $k$-means method \citep{[REF??]}, the classification in the middle panels (c, d, i, j) was obtained using the Spectral Clustering (SC) method \citep{[REF??]}, and the bottom two panels (e, f, k, l) feature a classification based on the Gaussian Mixture Model (GMM) \citep{[REF??]}.

The $k$-means technique has been used in a recent publication for the determination of solar wind states \citep{Roberts2020}. To our knowledge the SC and GMM methods have never been used in the literature to classify the solar wind, but \citep{Dupuis2020} has used the GMM to characterize magnetic reconnection regions in simulations using their velocity distribution information.

[REVIEW THIS PARAGRAHP??]
In this data set the most glaring issue is that different techniques lead to different clusters of points. Some similarities can be observed between the $k$-means technique and the SC method, but the GMM leads to clusters that do not seem to correlate with the previous two. The differences do not stop between clustering techniques: for a single method, e.g. $k$-means, slight modifications of the clustering parameters, e.g. using a different seed for the random number generator, can lead to very different results. Moreover, the cloud of points is convex and evenly distributed in all three components. This raises one additional issue, observed more clearly in columns 3 and 4 of Fig.\ref{fig:clustering}: when classical clustering methods are applied to homogeneously dense data, it divides the feature space in Vorono\"i regions with linear hyper-plane boundaries. Density-based clustering techniques, such as DBSCAN, \citep{[REF newDBSCAN paper??]}, do not suffer from this issue, but lead to a single class in homogeneous clouds.

There is no guarantee that a single classification method, with a particular set of parameters will converge to a physically meaningful classification of the data if the points in this data do not have some level of separability, or show clear zones of high density. This is also true for other classification methods based on `supervised learning'. The same issues will be observed if the training data uses target classes derived from dense data clouds using simple hyper-plane boundaries, as done for the Z and X classes (Table \ref{tab:swtypes}).

\subsubsection{Self-Organizing Maps}

\paragraph{Classical SOM}

Following the definitions and notations by \citep{[REF 23 in ROUGIER??]}, a class can be defined as $C_i\overset{\text{def}}{=} \{x\in\Omega | \Phi(x)=\boldsymbol{w}_i\}$, where $\Phi$ is a function from $\Omega$ to a finite subset of $k$ points $\{\boldsymbol{w}_i\in\Bbb R^N\}_{i=1..k}$. A cluster $C_i$ is then a partition of $\Omega$, and $\{\boldsymbol{w}_i\}$ are the code words (also known as nodes, weights or centroids) associated. The mapping from the data space to the code word set, $\Phi: \Omega\rightarrow\mathcal{W}$, is obtained by finding the closest neighbor, eq.\eqref{eq:winner}. The code word $\boldsymbol{w_x}$, the closest node to the input $\boldsymbol{x}$, is called the `winning element'. The class $C_i$ corresponds to a Vorono\"i region of $\Omega$ with center in $\boldsymbol{w}_i$.

\begin{equation}
\Phi: x \rightarrow  \underset{\arg\min}{i\in\mathcal{N}}\left( \left\lVert x - \boldsymbol{w}_i \right\rVert \right) \label{eq:winner}
\end{equation}

A Self-Organizing Map (SOM) is a collection of structured nodes arranged in a lattice, and assigned to a fixed position $\boldsymbol{p}_i$ in $\Bbb R^q$, where $q$ is the dimension of the lattice (generally $q=2$). The map nodes are characterized by their associated code words. The SOM learns by adjusting the code words $\boldsymbol{p}_i$ as input data $x$ is presented. The code word $s \in \mathcal{N}$ is associated to the winning element $\boldsymbol{w_x}$. At every iteration of the method, all code words of the SOM are shifted towards $x$ following the rule:

\begin{equation}
\Delta \boldsymbol{w}_i = \epsilon(t)h_\sigma(t,i,s)(x-\boldsymbol{w}_i) \label{eq:learnsom}
\end{equation}

with $h_\sigma(t,i,j)$ defined as the lattice neighbor function:

\begin{equation}
h_\sigma(t,i,j) = e^{-\frac{\left\lVert \boldsymbol{p}_i - \boldsymbol{p}_j \right\rVert^2}{2\sigma(t)^2}} \label{eq:neigsom}
\end{equation}

where $\epsilon(t)$ is the time dependent learning rate, eq.\eqref{eq:epsilon}, and $\sigma(t)$ is the time dependent lattice neighbor width, eq.\eqref{eq:sigma}. The training of the SOM is an iterative process where each data point in the data set is presented to the algorithm multiple times $t={0, 1,..,t_f}$. In these equations the subscript $0$ refers to initial values at $t=0$ and the subscript $f$ to values at $t=t_f$.

\begin{align}
\sigma(t) & = \sigma_0 \left(\frac{\sigma_f}{\sigma_0}\right)^{t/t_f} \label{eq:sigma} \\
\epsilon(t) & = \epsilon_0 \left(\frac{\epsilon_f}{\epsilon_0}\right)^{t/t_f} \label{eq:epsilon}
\end{align}

This procedure places the code words in the data space $\Omega$ in such a way that neighboring nodes in the lattice are also neighbors in the data space. The lattice can be presented as a $q$-dimensional image, called map, where nodes sharing similar properties are organized in close proximity.

The main metric for the evaluation of the performance of the SOM is called the quantization error:

\begin{equation}
Q_E = \frac{1}{M} \sum_{i=1}^M \left\lVert x_i - w_{x_i} \right\rVert
\end{equation}

where $M$, is the total number of entries in the data set.

Once the training of the SOM is finished, the code words $\boldsymbol{w}_i$ can be grouped together using one of the previously described clustering techniques, e.g. k-means. The nodes of the SOM with close properties will be made part of the same class. The classes thus created are an ensemble of Vorono\"i subspaces, allowing a partitioning of the data space $\Omega$. The number of clusters is an input of the algorithm, but can also be calculated autonomously. The Within Cluster Sum of Squares (WCSS) can be used as a metric of the compactness of the clustered nodes. As its name implies the WCSS is the sum of the squre distances from each node to their cluster point. If one class is used, the large spread of the nodes produces a high WCSS. The lowest value would be obtained for a very high number of classes, but that would be unpractical. The optimal number of clusters can be obtained using the Kneedle class number determination\citep{Satopaa2011}. In the present work we avoid using this technique in order to perform comparisons with previous publications, but we will explore the use of an automatic class number selection in a future manuscript.

\paragraph{Dynamic SOM}
The time dependence of the SOM training allows the code words $\boldsymbol{w}_i$ to reach steady coordinates by slowing down their movement over the iterations. Due to the minimization of the distance in eq.\eqref{eq:winner} code words tend to agglomerate around high density zones of the feature space. The Dynamic Self-Organizing Maps (DSOM), introduced by \citep{Rougier2011}, eliminate the time dependence and allows to cover larger zones of the space outside of the high density regions.

The DSOM is a variation of the SOM where the learning function \eqref{eq:learnsom} and the neighbor function \eqref{eq:neigsom} are replaced by eqs. \eqref{eq:learndsom} and \eqref{eq:neigdsom} respectively:

\begin{align}
\Delta \boldsymbol{w}_i & = \epsilon \left\lVert x - \boldsymbol{w}_i \right\rVert_\Omega h_\eta(i,s,x)(x-\boldsymbol{w}_i) \label{eq:learndsom} \\
h_\eta(i,s,x) & = e^{-\frac{1}{\eta^2}\frac{\left\lVert \boldsymbol{p}_i - \boldsymbol{p}_j \right\rVert^2}{\left\lVert x - \boldsymbol{w}_s \right\rVert_\Omega^2}} \label{eq:neigdsom} 
\end{align}

where $\epsilon$ is a constant learning rate, $h_\sigma(t,i,s)$ is defined as the new lattice neighbor function, and $\eta$ is the `elasticity' parameter. In his work \citep{Rougier2011} shows that DSOM can be used to better sample the feature space $\Omega$, reducing the agglomeration of code words around high density zones. The DSOM does not converge to a steady solution, due to the lack of a temporal damping factor.

\paragraph{Visualization of SOM and DSOM}
Clustering techniques do not convergence to a steady immutable solution. Differences in the training parameters or slight changes in the data can have an important impact on the final classification. These tools can be used for statistical analysis, comparisons, data visualization and training of supervised methods. But it will be practically impossible to claim the existence of a general objective set of states discovered only by the use of these basic clustering techniques.

However, SOMs and DSOMs provide an important tool for the study of the solar wind: the maps are composed of nodes that share similar properties with its immediate neighbors. This allows for visual identification of patterns and targeted statistical analysis.

Fig.\ref{fig:maps} shows the basic types of plots and maps that can be generated using the SOM/DSOM techniques. The top left panel shows a histogram of the first two components of the feature space $\Omega$, with dots marking the position of the code words $\boldsymbol{w}_i$. The colors of the dots represent the normalized sum of all their components, $r_i =$ norm$(R_i)$, where $R_i = \sum_{k=1}^q w_{k,i}$. The red lines connect a single code word $\boldsymbol{w}_s$ with its six closest neighbors. The second panel, shows the `hit map' of the SOM. It contains the lattice nodes $\boldsymbol{p}_i$ associated to the code words $\boldsymbol{w}_i$. They are represented by hexagons with the same color as the code words. The size of the hexagon is proportional to the number of data points connected to the each node. The thickness of the lines between lattice nodes represent the relative distance to its neighbors in the feature space $\Omega$. Red lines connect the node $\boldsymbol{p}_s$ to its closest neighbors, showing the connection with the plot in the first panel.

The third panel of the top row of Fig.\ref{fig:maps} corresponds to the value of a single data feature, the ionized oxygen ratio $O^{7+}/O^{7+}$ (`O7to6'), associated to each of the nodes (code words). To improve visualization all hexagon sizes have been set to their maximum and the inter-node distance line has been colored white. In order to obtain the correct values for each node, we must first perform a dimension inversion of the data followed by a re-scaling. If the data reduction was performed using PCA, we perform a inverse transformation, and if the feature space was generated with an autoencoder, we perform a node decoding.

The fourth panel in the top row of Fig.\ref{fig:maps} shows that the nodes of the lattice can also represent data that has not been used in the training of the SOM. Knowing the points in the data set associated to a particular node, it is possible to perform independent statistical operations on that subsection of data. In this case we have colored the map using the average oxygen charge state $\left<Q_{O}\right>$ (`avqO'), and the size of the nodes represent the frequency of points with solar wind type X=2 (ejecta).

The bottom row of Fig.\ref{fig:maps} displays all three components of the $\boldsymbol{p}_i$ nodes. In the first panel they have been mapped to the basic colors Red, Green and Blue (RGB). The remaining panels have been colored using each individual component. The first panel is the RGB composition of the three remaining ones.

\subsubsection{The Full Architecture}
The previous sections introduced all the components used for the present work. Here we give a global view of the full model. Fig.\ref{fig:architecture} shows how all the components are interconnected. The data set is composed of clean and processed entries. When the number of features is large, the data is transformed by a linear transformation, using PCA, or by a non-linear encoding, using autoencoders. The transformed data is then used to train the DSOM. After training, the code words of the SOM can be classified to group together nodes that share similar properties. The total number of classes is an additional input of the system.

\paragraph{Autoencoder architecture}
We use a basic, fully connected feed-forward neural network for the encoding-decoding process. The bottleneck of the network has been fixed to three neurons. The neural network is symmetric in size but the weights of the encoder and the decoder are not synchronized. We use a total of five hidden layers (including the bottleneck). Each layer is composed of a linear regressor, followed by a 1D batch normalization and a ReLU activation function. The output layer of the network contains a linear regressor followed by a hyperbolic tangent activation function. The autoencoder has been coded in python using the PyTorch framework.

\paragraph{Four Models of Solar Wind Classification}
We have tested the four models presented in Table \ref{tab:features}. Three of the models are inspired by the work of Roberts (R) \citep{Roberts2020}, Xu and Borovsky (X) \citep{[XU??]}, Zhao et al (Z) \citep{Zhao2009}. We include one additional model (A). The table details the features used in each model. Details on the description of each feature can be found in the \href{http://www.srl.caltech.edu/cgi-bin/dib/rundibviewmultil2/ACE/ASC/DATA/level2/multi}{ACE Level 2 documentation online}. To avoid low variance we have used the logarithm of all the quantities, except those marked with an asterisk in the table.

Features 16 to 25 contain an additional suffix, corresponding to a statistical operation performed on the corresponding feature. The operations include the mean, the range, the standard deviation and the auto-correlation of quantities over a window of time of 6 hours.

On the lower part of Table \ref{tab:features}, the range of dates used for each model reflects the data used in the corresponding original publications. We apply PCA to models A, R and X, and we test the autoencoding only on models A and R. The number of neurons in the encoding half is listed in the table.

Up until this point all the figures presented correspond to the results obtained using model A. The amount of data and figures produced in this work is very large and is not possible to fit completely in the present document. We will present some highlights in this manuscript, but more detailed analysis of each one of the cases will have to be presented in future publications.

\paragraph{Budget}
Machine learning models require fine tuning of different parameters, from the selection and testing of multiple methods, to the parameterization of the final architecture. \citep{[REF??]} suggests that every publication in machine learning should include a section on the budget used for the development and training of the method. The budget is the amount of resources used in the data processing, the selection of the model hyper-parameters (HP), and its training.

The most time-consuming task in the present work has been the data preparation, the model setup and debugging and the writing of the SOM visualization routines. All the techniques described in the Methods section \ref{sec:methods} have been coded in python and are freely accessible in the repositories listed in section \ref{sec:repos}. We estimate the effort to bring this work from scratch to a total of 2 persons month. Of these, one person week was dedicated to the testing an selection of different model HPs (autoencoder architecture, feature selection, learning rates, initialization methods, number of epochs for training, selection of data compression method, size of the time windows, etc.).

All classic clustering techniques presented in section \ref{sec:clustering} require only a few lines of code and can be trained in less than a minute on a current generation laptop (e.g. Dell XPS 13 2016 with an Intel Core i7-7500U CPU). The most time consuming task is the training of the autoencoder which can last for up to 3 minutes. The training of the SOM is performed in less than a minute.

\paragraph{Hyper-Parameter Optimization}
We are focusing in this work on the use of the SOMs. These require the selection of four main HPs: the size of the lattice, $(m\times n)$, the initial learning rate, $\epsilon_0$HPs are replaced by the constant learning rate, $\epsilon$, and the elasticity, $\eta$. The automatic selection of the best HP for machine learning model is called Hyper-Parameter Optimization (HPO).

We use the library `optuna' \citep{[REF??]} to perform an automatic optimization of the four HPs. The optimization is based on a technique called Tree-structured Parzen Estimator (TPE) \citep{[REF 3 IN OPTUNA PAPER??]}. The objective function of the optimizer was set to minimize the quantization error $Q_E$, the mean inter nodal distance, and the elasticity (or the neighborhood width) of the SOM. After a total of 100 trials (runs of the model using different HPs), the optimizer selected the parameters presented in table \ref{tab:hpo}. The total run-time for a single HPO is in the order of 30 minutes on a home computer. HPO is, understandably, the most expensive procedure.