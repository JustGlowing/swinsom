@article{Suveges2017,
author = {S{\"{u}}veges, M. and Barblan, F. and Lecoeur-Ta{\"{i}}bi, I. and Pr{\v{s}}a, A. and Holl, B. and Eyer, L. and Kochoska, A. and Mowlavi, N. and Rimoldini, L.},
doi = {10.1051/0004-6361/201629710},
file = {:home/amaya/MEGAsync/Mendeley/S{\"{u}}veges et al. - 2017 - iGaiai eclipsing binary and multiple systems. Supervised classification and self-organizing maps.pdf:pdf},
issn = {0004-6361},
journal = {Astronomy {\&} Astrophysics},
month = {jul},
pages = {A117},
title = {{{\textless}i{\textgreater}Gaia{\textless}/i{\textgreater} eclipsing binary and multiple systems. Supervised classification and self-organizing maps}},
url = {http://www.aanda.org/10.1051/0004-6361/201629710},
volume = {603},
year = {2017}
}
@inproceedings{ester1996density,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei and Others},
booktitle = {Kdd},
file = {:home/amaya/MEGAsync/Mendeley/Ester et al. - 1996 - A density-based algorithm for discovering clusters in large spatial databases with noise.pdf:pdf},
number = {34},
pages = {226--231},
title = {{A density-based algorithm for discovering clusters in large spatial databases with noise.}},
volume = {96},
year = {1996}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/amaya/MEGAsync/Mendeley/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Dodge2019,
abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
archivePrefix = {arXiv},
arxivId = {1909.03004},
author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
eprint = {1909.03004},
file = {:home/amaya/MEGAsync/Mendeley/Dodge et al. - 2019 - Show Your Work Improved Reporting of Experimental Results.pdf:pdf},
month = {sep},
title = {{Show Your Work: Improved Reporting of Experimental Results}},
url = {http://arxiv.org/abs/1909.03004},
year = {2019}
}
@article{Villmann2006,
abstract = {We consider different ways to control the magnification in self-organizing maps (SOM) and neural gas (NG). Starting from early approaches of magnification control in vector quantization, we then concentrate on different approaches for SOM and NG. We show that three structurally similar approaches can be applied to both algorithms that are localized learning, concave-convex learning, and winner-relaxing learning. Thereby, the approach of concave-convex learning in SOM is extended to a more general description, whereas the concave-convex learning for NG is new. In general, the control mechanisms generate only slightly different behavior comparing both neural algorithms. However, we emphasize that the NG results are valid for any data dimension, whereas in the SOM case, the results hold only for the one-dimensional case.},
author = {Villmann, Thomas and Claussen, Jens Christian},
doi = {10.1162/089976606775093918},
file = {:home/amaya/MEGAsync/Mendeley/Villmann, Claussen - 2006 - Magnification Control in Self-Organizing Maps and Neural Gas.pdf:pdf},
journal = {Neural Computation},
number = {2},
pages = {446--469},
title = {{Magnification Control in Self-Organizing Maps and Neural Gas}},
url = {https://doi.org/10.1162/089976606775093918},
volume = {18},
year = {2006}
}
@article{Villmann2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0609517v1},
author = {Villmann, Thomas and Claussen, Jens Christian},
eprint = {0609517v1},
file = {:home/amaya/MEGAsync/Mendeley/Villmann, Claussen - 2006 - Magnification Control in Self-Organizing Maps and Neural Gas.pdf:pdf},
pages = {446--469},
primaryClass = {arXiv:cond-mat},
title = {{1 Introduction}},
volume = {469},
year = {2006}
}
@article{bishop2006machine,
author = {Bishop, Christopher M},
journal = {Information science and statistics. Springer, Heidelberg},
title = {{Machine learning and pattern recognition}},
year = {2006}
}
@article{1056489,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\^{}}{\{}b{\}}quanta,b=1,2, $\backslash$cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
author = {Lloyd, S},
doi = {10.1109/TIT.1982.1056489},
file = {:home/amaya/MEGAsync/Mendeley/Lloyd - 1982 - Least squares quantization in PCM.pdf:pdf},
issn = {1557-9654},
journal = {IEEE Transactions on Information Theory},
keywords = {Least-squares approximation,PCM communication,Quantization (signal),Signal quantization},
month = {mar},
number = {2},
pages = {129--137},
title = {{Least squares quantization in PCM}},
volume = {28},
year = {1982}
}
@article{Dupuis2020,
author = {Dupuis, Romain and Goldman, Martin V. and Newman, David L. and Amaya, Jorge and Lapenta, Giovanni},
doi = {10.3847/1538-4357/ab5524},
issn = {1538-4357},
journal = {The Astrophysical Journal},
month = {jan},
number = {1},
pages = {22},
title = {{Characterizing Magnetic Reconnection Regions Using Gaussian Mixture Models on Particle Velocity Distributions}},
url = {https://iopscience.iop.org/article/10.3847/1538-4357/ab5524},
volume = {889},
year = {2020}
}
@article{Goodfellow2014,
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
file = {:home/amaya/MEGAsync/Mendeley/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf},
month = {jun},
title = {{Generative Adversarial Networks}},
url = {https://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Welling,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6114v10},
author = {Welling, Max},
eprint = {arXiv:1312.6114v10},
file = {:home/amaya/MEGAsync/Mendeley/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes arXiv : 1312 . 6114v10 [ stat . ML ] 1 May 2014}}
}
@inproceedings{7407967,
abstract = {Deep neural network has very strong nonlinear mapping capability, and with the increasing of the numbers of its layers and units of a given layer, it would has more powerful representation ability. However, it may cause very serious overfitting problem and slow down the training and testing procedure. Dropout is a simple and efficient way to prevent overfitting. We combine stacked denoising autoencoder and dropout together, then it has achieved better performance than singular dropout method, and has reduced time complexity during fine-tune phase. We pre-train the data with stacked denoising autoencoder, and to prevent units from co-adapting too much dropout is applied in the period of training. At test time, it approximates the effect of averaging the predictions of many networks by using a network architecture that shares the weights. We show the performance of this method on a common benchmark dataset MNIST.},
author = {Liang, J and Liu, R},
booktitle = {2015 8th International Congress on Image and Signal Processing (CISP)},
doi = {10.1109/CISP.2015.7407967},
keywords = {encoding;neural nets;signal denoising;stacked deno},
month = {oct},
pages = {697--701},
title = {{Stacked denoising autoencoder and dropout together to prevent overfitting in deep neural network}},
year = {2015}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:home/amaya/MEGAsync/Mendeley/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@inproceedings{5961514,
abstract = {Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These "knees'' typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.},
author = {Satopaa, V and Albrecht, J and Irwin, D and Raghavan, B},
booktitle = {2011 31st International Conference on Distributed Computing Systems Workshops},
doi = {10.1109/ICDCSW.2011.20},
issn = {2332-5666},
keywords = {performance evaluation;systems analysis;computer s},
month = {jun},
pages = {166--171},
title = {{Finding a "Kneedle" in a Haystack: Detecting Knee Points in System Behavior}},
year = {2011}
}
@article{Cane2003,
abstract = {We summarize the occurrence of interplanetary coronal mass ejections (ICMEs) in the near-Earth solar wind during 1996-2002, corresponding to the increasing and maximum phases of solar cycle 23. In particular, we give a detailed list of such events. This list, based on in situ observations, is not confined to subsets of ICMEs, such as "magnetic clouds" or those preceded by "halo" coronal mass injections (CMEs) observed by the Solar and Heliospheric Observatory/Large Angle and Spectrometric Coronagraph, and provides an overview of 214 ICMEs in the near-Earth solar wind during this period. The ICME rate increases by about an order of magnitude from solar minimum to solar maximum (when the rate is ∼3 ICMEs per solar rotation period). The rate also shows a temporary reduction during 1999 and another brief, deeper reduction in late 2000 to early 2001, which only approximately track variations in the solar 10-cm flux. In addition, there are occasional periods of several rotations duration when the ICME rate is enhanced in association with high solar activity levels. We find an indication of a periodic variation in the ICME rate, with a prominent period of ∼165 days similar to that previously reported in various solar phenomena. It is found that the fraction of ICMEs that are magnetic clouds has a solar cycle variation, the fraction being larger near solar minimum. For the subset of events that we could associate with a CME at the Sun the transit speeds from the Sun to the Earth were highest after solar maximum. Copyright 2003 by the American Geophysical Union.},
author = {Cane, H. V. and Richardson, I. G.},
doi = {10.1029/2002JA009817},
file = {:home/amaya/MEGAsync/Mendeley/Cane, Richardson - 2003 - Interplanetary coronal mass ejections in the near-Earth solar wind during 1996-2002.pdf:pdf},
issn = {21699402},
journal = {Journal of Geophysical Research: Space Physics},
keywords = {Coronal mass ejections,Interplanetary coronal mass ejections,Magnetic clouds,Solar cycle variation,Solar wind},
number = {A4},
title = {{Interplanetary coronal mass ejections in the near-Earth solar wind during 1996-2002}},
volume = {108},
year = {2003}
}
@article{Richardson2010,
abstract = {In a previous study (Cane and Richardson, J. Geophys. Res. 108(A4), SSH6-1, 2003), we investigated the occurrence of interplanetary coronal mass ejections in the near-Earth solar wind during 1996 - 2002, corresponding to the increasing and maximum phases of solar cycle 23, and provided a "comprehensive" catalog of these events. In this paper, we present a revised and updated catalog of the ≈300 near-Earth ICMEs in 1996 - 2009, encompassing the complete cycle 23, and summarize their basic properties and geomagnetic effects. In particular, solar wind composition and charge state observations are now considered when identifying the ICMEs. In general, these additional data confirm the earlier identifications based predominantly on other solar wind plasma and magnetic field parameters. However, the boundaries of ICME-like plasma based on charge state/composition data may deviate significantly from those based on conventional plasma/magnetic field parameters. Furthermore, the much studied "magnetic clouds", with flux-rope-like magnetic field configurations, may form just a substructure of the total ICME interval. {\textcopyright} 2010 Springer Science+Business Media B.V.},
author = {Richardson, I. G. and Cane, H. V.},
doi = {10.1007/s11207-010-9568-6},
file = {:home/amaya/MEGAsync/Mendeley/Richardson, Cane - 2010 - Near-earth interplanetary coronal mass ejections during solar cycle 23 (1996 - 2009) Catalog and summary of pr.pdf:pdf},
issn = {00380938},
journal = {Solar Physics},
keywords = {Coronal mass ejections,Interplanetary coronal mass ejections,Interplanetary magnetic field,Magnetic clouds,Solar wind plasma},
number = {1},
pages = {189--237},
title = {{Near-earth interplanetary coronal mass ejections during solar cycle 23 (1996 - 2009): Catalog and summary of properties}},
volume = {264},
year = {2010}
}
@article{PhysRev.79.183,
author = {Elsasser, Walter M},
doi = {10.1103/PhysRev.79.183},
file = {:home/amaya/MEGAsync/Mendeley/Elsasser - 1950 - The Hydromagnetic Equations.pdf:pdf},
journal = {Phys. Rev.},
month = {jul},
number = {1},
pages = {183},
publisher = {American Physical Society},
title = {{The Hydromagnetic Equations}},
url = {https://link.aps.org/doi/10.1103/PhysRev.79.183},
volume = {79},
year = {1950}
}
@article{Holden2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Holden, A J and Robbins, D J and Stewart, W J and Smith, D R and Schultz, S and Wegener, M and Linden, S and Hormann, C and Enkrich, C and Soukoulis, C M and Schurig, D and Taylor, A J and Highstrete, C and Lee, M and Averitt, R D and Markos, P and Mcpeake, D and Ramakrishna, S A and Pendry, J B and Shalaev, V M and Maksimchuk, M and Umstadter, D and Chen, W and Shen, Y R and Moloney, J V},
file = {:home/amaya/MEGAsync/Mendeley/Hinton, Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
number = {July},
pages = {504--507},
title = {{Reducing the Dimensionality of}},
volume = {313},
year = {2006}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:home/amaya/MEGAsync/Mendeley/Hinton, Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
journal = {Science},
month = {jul},
number = {5786},
pages = {504 LP -- 507},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://science.sciencemag.org/content/313/5786/504.abstract},
volume = {313},
year = {2006}
}
@article{Shlens2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.1100v1},
author = {Shlens, Jonathon and View, Mountain and Introduction, I},
eprint = {arXiv:1404.1100v1},
file = {:home/amaya/MEGAsync/Mendeley/Shlens, View, Introduction - 2014 - A Tutorial on Principal Component Analysis.pdf:pdf},
title = {{A Tutorial on Principal Component Analysis}},
year = {2014}
}
@article{Zhao2009,
abstract = {The composition of the solar wind can be used to determine its origin at the Sun; e.g., solar wind from coronal holes has demonstrably lower charge states than solar wind of other origins. The O7+/O6+ ratio as measured by Advanced Composition Explorer (ACE) during 1998-2008 is used to divide the solar wind into three categories: non-transient solar wind from coronal holes (hereafter referred to as CHW); non-transient solar wind that originates from outside of coronal holes (hereafter referred to as NCHW), and solar wind associated with transient interplanetary coronal mass ejections (ICMEs). The global distribution of the solar wind relative to the Heliospheric Current Sheet (HCS), as specified by a Potential-Field-Source-Surface model, is then determined. The solar wind from outside of coronal holes is found to originate from a band of about 40° in width about the HCS during solar maximum conditions, and a much smaller band of {\textless} 17° during solar minimum. These results are consistent with models for the global transport of the solar magnetic field during the solar cycle, and they are consistent with earlier global flow structure determinations based upon velocity alone Copyright 2009 by the American Geophysical Union.},
author = {Zhao, L. and Zurbuchen, T. H. and Fisk, L. A.},
doi = {10.1029/2009GL039181},
file = {:home/amaya/MEGAsync/Mendeley/Zhao, Zurbuchen, Fisk - 2009 - Global distribution of the solar wind during solar cycle 23 ACE observations.pdf:pdf},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {http://dx.doi.org/10.1029/2009GL039181, doi:10.102},
number = {14},
pages = {1--4},
title = {{Global distribution of the solar wind during solar cycle 23: ACE observations}},
volume = {36},
year = {2009}
}
@inproceedings{pmlr-v28-bergstra13,
abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.},
address = {Atlanta, Georgia, USA},
author = {Bergstra, James and Yamins, Daniel and Cox, David},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
editor = {Dasgupta, Sanjoy and McAllester, David},
file = {:home/amaya/MEGAsync/Mendeley/Bergstra, Yamins, Cox - 2013 - Making a Science of Model Search Hyperparameter Optimization in Hundreds of Dimensions for Vision Archite.pdf:pdf},
number = {1},
pages = {115--123},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures}},
url = {http://proceedings.mlr.press/v28/bergstra13.html},
volume = {28},
year = {2013}
}
@article{Zhao2018,
abstract = {We use OMNI 1-minute resolution data sets from 1995 through 2017, covering about two consecutive solar cycles, to investigate the solar cycle dependence of various turbulence quantities and cosmic ray (CR) mean free paths. We employ quasi-linear theory (QLT) and nonlinear guiding center theory (NLGC) to evaluate the CR parallel and perpendicular diffusion. We find that in the ecliptic plane at 1 au (1) the fluctuating magnetic energy density z ±2, residual energy ED , and corresponding correlation functions all have an obvious solar cycle dependence. The residual energy ED is always negative, which indicates that the energy in magnetic fluctuations is larger than the energy in kinetic fluctuations, especially at solar maximum; (2) the correlation length $\lambda$ for magnetic fluctuations does not show significant solar cycle variation; (3) the temporally varying shear source of turbulence, which is most important in the inner heliosphere, depends on the solar cycle; and (4) high level turbulence will increase CR perpendicular diffusion and decrease CR parallel diffusion, but this trend can be masked if the background interplanetary magnetic field (IMF) changes in concert with turbulence in response to solar activity. These results provide quantitative inputs for both turbulence transport models and CR diffusion coefficient models.},
author = {Zhao, L. L. and Adhikari, L. and Zank, G. P. and Hu, Q. and Feng, X. S.},
doi = {10.1088/1742-6596/1100/1/012029},
file = {:home/amaya/MEGAsync/Mendeley/Zhao et al. - 2018 - Analytical investigation of turbulence quantities and cosmic ray mean free paths from 1995-2017.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {1},
title = {{Analytical investigation of turbulence quantities and cosmic ray mean free paths from 1995-2017}},
volume = {1100},
year = {2018}
}
@article{Roberts2020,
author = {Roberts, D. Aaron and Karimabadi, Homa and Sipes, Tamara and Ko, Yuan-Kuen and Lepri, Susan},
doi = {10.3847/1538-4357/ab5a7a},
file = {:home/amaya/MEGAsync/Mendeley/Roberts et al. - 2020 - Objectively Determining States of the Solar Wind Using Machine Learning.pdf:pdf},
issn = {1538-4357},
journal = {The Astrophysical Journal},
keywords = {Interplanetary physics,Solar wind,Interplanetary t},
number = {2},
pages = {153},
publisher = {IOP Publishing},
title = {{Objectively Determining States of the Solar Wind Using Machine Learning}},
url = {http://dx.doi.org/10.3847/1538-4357/ab5a7a},
volume = {889},
year = {2020}
}
@article{Magyar2019,
abstract = {The Els$\backslash$"{\{}a{\}}sser variables are often used in studies of plasma turbulence, in helping differentiate between MHD waves propagating parallel or anti-parallel to the main magnetic field. While for pure Alfv$\backslash$'en waves in a homogeneous plasma the method is strictly valid, we show that compressible, magnetoacoustic waves are in general described by both Els$\backslash$"{\{}a{\}}sser variables. Furthermore, in a compressible and inhomogeneous plasma, the pure MHD waves (Alfv$\backslash$'en, fast and slow) are no longer normal modes, but waves become linearly coupled or display mixed properties of Alfv$\backslash$'en and magnetoacoustic nature. These waves are necessarily described by both Els$\backslash$"{\{}a{\}}sser variables and therefore the Els$\backslash$"{\{}a{\}}sser formalism cannot be used to strictly separate parallel and anti-parallel propagating waves. Nevertheless, even in an inhomogeneous plasma, for a highly Alfv$\backslash$'enic wave the Els$\backslash$"{\{}a{\}}sser variable corresponding to the propagation direction appears still dominating. We suggest that for Alfv$\backslash$'enic waves, the relative amplitude of Els$\backslash$"{\{}a{\}}sser variables depends on the local degree of inhomogeneity and other plasma and wave properties. This finding has implications for turbulence studies in inhomogeneous and compressible plasmas, such as the solar corona and solar wind.},
archivePrefix = {arXiv},
arxivId = {1902.01619},
author = {Magyar, N. and {Van Doorsselaere}, T. and Goossens, M.},
doi = {10.3847/1538-4357/ab04a7},
eprint = {1902.01619},
file = {:home/amaya/MEGAsync/Mendeley/Magyar, Van Doorsselaere, Goossens - 2019 - The Nature of Els{\"{a}}sser Variables in Compressible MHD.pdf:pdf},
issn = {1538-4357},
journal = {The Astrophysical Journal},
keywords = {animations,magnetohydrodynamics,magnetohydrodynamics (MHD),solar wind,turbulence,w,mhd,solar wind,supporting material,turbulence,waves},
number = {1},
pages = {56},
publisher = {IOP Publishing},
title = {{The Nature of Els{\"{a}}sser Variables in Compressible MHD}},
url = {http://dx.doi.org/10.3847/1538-4357/ab04a7},
volume = {873},
year = {2019}
}
@article{Akiba2019,
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to lightweight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
archivePrefix = {arXiv},
arxivId = {1907.10902},
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
doi = {10.1145/3292500.3330701},
eprint = {1907.10902},
file = {:home/amaya/MEGAsync/Mendeley/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimization Framework.pdf:pdf},
isbn = {9781450362016},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Bayesian optimization,Black-box optimization,Hyperparameter optimization,Machine learning system},
pages = {2623--2631},
title = {{Optuna: A Next-generation Hyperparameter Optimization Framework}},
year = {2019}
}
@article{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
file = {:home/amaya/MEGAsync/Mendeley/Bergstra et al. - 2011 - Algorithms for hyper-parameter optimization.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
pages = {1--9},
title = {{Algorithms for hyper-parameter optimization}},
year = {2011}
}
@article{Adhikari2020,
abstract = {Parker Solar Probe (PSP) achieved its first orbit perihelion on November 6, 2018, reaching a heliocentric distance of about 0.165 au (35.55 R{\$}{\_}\backslashodot{\$}). Here, we study the evolution of fully developed turbulence associated with the slow solar wind along the PSP trajectory between 35.55 R{\$}{\_}\backslashodot{\$} and 131.64 R{\$}{\_}\backslashodot{\$} in the outbound direction, comparing observations to a theoretical turbulence transport model. Several turbulent quantities, such as the fluctuating kinetic energy and the corresponding correlation length, the variance of density fluctuations, and the solar wind proton temperature are determined from the PSP SWEAP plasma data along its trajectory between 35.55 R{\$}{\_}\backslashodot{\$} and 131.64 R{\$}{\_}\backslashodot{\$}. The evolution of the PSP derived turbulent quantities are compared to the numerical solutions of the nearly incompressible magnetohydrodynamic (NI MHD) turbulence transport model recently developed by Zank et al. (2017). We find reasonable agreement between the theoretical and observed results. On the basis of these comparisons, we derive other theoretical turbulent quantities, such as the energy in forward and backward propagating modes, the total turbulent energy, the normalized residual energy and cross-helicity, the fluctuating magnetic energy, and the correlation lengths corresponding to forward and backward propagating modes, the residual energy, and the fluctuating magnetic energy.},
archivePrefix = {arXiv},
arxivId = {1912.02372},
author = {Adhikari, L. and Zank, G. P. and Zhao, L.-L. and Kasper, J. C. and Korreck, K. E. and Stevens, M. and Case, A. W. and Whittlesey, P. and Larson, D. and Livi, R. and Klein, K. G.},
doi = {10.3847/1538-4365/ab5852},
eprint = {1912.02372},
file = {:home/amaya/MEGAsync/Mendeley/Adhikari et al. - 2020 - Turbulence Transport Modeling and First Orbit Parker Solar Probe ( PSP ) Observations.pdf:pdf},
issn = {1538-4365},
journal = {The Astrophysical Journal Supplement Series},
keywords = {Interplanetary turbulence,Slow solar wind,Solar wi},
number = {2},
pages = {38},
publisher = {IOP Publishing},
title = {{ Turbulence Transport Modeling and First Orbit Parker Solar Probe ( PSP ) Observations }},
url = {http://dx.doi.org/10.3847/1538-4365/ab5852},
volume = {246},
year = {2020}
}
@article{Chen2020,
abstract = {The first two orbits of the Parker Solar Probe (PSP) spacecraft have enabled the first in situ measurements of the solar wind down to a heliocentric distance of 0.17 au (or 36 Rs). Here, we present an analysis of this data to study solar wind turbulence at 0.17 au and its evolution out to 1 au. While many features remain similar, key differences at 0.17 au include: increased turbulence energy levels by more than an order of magnitude, a magnetic field spectral index of -3/2 matching that of the velocity and both Elsasser fields, a lower magnetic compressibility consistent with a smaller slow-mode kinetic energy fraction, and a much smaller outer scale that has had time for substantial nonlinear processing. There is also an overall increase in the dominance of outward-propagating Alfv$\backslash$'enic fluctuations compared to inward-propagating ones, and the radial variation of the inward component is consistent with its generation by reflection from the large-scale gradient in Alfv$\backslash$'en speed. The energy flux in this turbulence at 0.17 au was found to be {\~{}}10{\%} of that in the bulk solar wind kinetic energy, becoming {\~{}}40{\%} when extrapolated to the Alfv$\backslash$'en point, and both the fraction and rate of increase of this flux towards the Sun is consistent with turbulence-driven models in which the solar wind is powered by this flux.},
archivePrefix = {arXiv},
arxivId = {1912.02348},
author = {Chen, C. H. K. and Bale, S. D. and Bonnell, J. W. and Borovikov, D. and Bowen, T. A. and Burgess, D. and Case, A. W. and Chandran, B. D. G. and de Wit, T. Dudok and Goetz, K. and Harvey, P. R. and Kasper, J. C. and Klein, K. G. and Korreck, K. E. and Larson, D. and Livi, R. and MacDowall, R. J. and Malaspina, D. M. and Mallet, A. and McManus, M. D. and Moncuquet, M. and Pulupa, M. and Stevens, M. L. and Whittlesey, P.},
doi = {10.3847/1538-4365/ab60a3},
eprint = {1912.02348},
file = {:home/amaya/MEGAsync/Mendeley/Chen et al. - 2020 - The Evolution and Role of Solar Wind Turbulence in the Inner Heliosphere.pdf:pdf},
issn = {1538-4365},
journal = {The Astrophysical Journal Supplement Series},
keywords = {Solar wind,Space plasmas,Interplanetary turbulence},
number = {2},
pages = {53},
publisher = {IOP Publishing},
title = {{The Evolution and Role of Solar Wind Turbulence in the Inner Heliosphere}},
url = {http://dx.doi.org/10.3847/1538-4365/ab60a3},
volume = {246},
year = {2020}
}
@article{Rougier2011,
abstract = {We present in this paper a variation of the self-organising map algorithm where the original time-dependent (learning rate and neighbourhood) learning function is replaced by a time-invariant one. This allows for on-line and continuous learning on both static and dynamic data distributions. One of the property of the newly proposed algorithm is that it does not fit the magnification law and the achieved vector density is not directly proportional to the density of the distribution as found in most vector quantisation algorithms. From a biological point of view, this algorithm sheds light on cortical plasticity seen as a dynamic and tight coupling between the environment and the model.},
author = {Rougier, Nicolas and Boniface, Yann},
doi = {10.1016/J.NEUCOM.2010.06.034},
file = {:home/amaya/MEGAsync/Mendeley/Rougier, Boniface - 2011 - Dynamic self-organising map.pdf:pdf},
issn = {0925-2312},
journal = {Neurocomputing},
month = {may},
number = {11},
pages = {1840--1847},
publisher = {Elsevier},
title = {{Dynamic self-organising map}},
url = {https://www.sciencedirect.com/science/article/pii/S0925231211000713},
volume = {74},
year = {2011}
}
@book{Camporeale2018,
author = {Camporeale, Enrico. and Wing, Simon. and Johnson, Jay R.},
file = {:home/amaya/MEGAsync/Mendeley/Camporeale, Wing, Johnson - 2018 - Machine learning techniques for space weather.pdf:pdf},
isbn = {9780128117880},
publisher = {Elsevier},
title = {{Machine learning techniques for space weather}},
year = {2018}
}
@article{Xu2015b,
author = {Xu, Fei and Borovsky, Joseph E.},
doi = {10.1002/2014JA020412},
file = {:home/amaya/MEGAsync/Mendeley/Xu, Borovsky - 2015 - A new four-plasma categorization scheme for the solar wind.pdf:pdf},
isbn = {2169-9402},
issn = {21699380},
journal = {Journal of Geophysical Research: Space Physics},
keywords = {coronal holes,solar cycle,solar wind source,streamer belt},
month = {jan},
number = {1},
pages = {70--100},
title = {{A new four-plasma categorization scheme for the solar wind}},
url = {http://doi.wiley.com/10.1002/2014JA020412},
volume = {120},
year = {2015}
}
